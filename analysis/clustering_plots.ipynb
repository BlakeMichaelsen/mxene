{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87bdaec9-551f-4cd0-8e7c-cf256fd199fe",
   "metadata": {},
   "source": [
    "## Necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eadee4b-e3fc-4dd6-9709-a92a0d91f5a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gsd.hoomd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import gspread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d07d2bcc-875d-4a60-82d4-2cda752b53ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_all_flake_coms(flake_positions, box_lengths, beads_per_flake=50):\n",
    "    Lx, Ly, Lz = box_lengths\n",
    "    n_flakes = len(flake_positions) // beads_per_flake\n",
    "    flake_coms = []\n",
    "\n",
    "    for i in range(n_flakes):\n",
    "        flake = flake_positions[i * beads_per_flake : (i + 1) * beads_per_flake]\n",
    "        ref = flake[0]\n",
    "        delta = flake - ref\n",
    "\n",
    "        # Minimum image convention\n",
    "        delta[:, 0] -= Lx * np.round(delta[:, 0] / Lx)\n",
    "        delta[:, 1] -= Ly * np.round(delta[:, 1] / Ly)\n",
    "        delta[:, 2] -= Lz * np.round(delta[:, 2] / Lz)\n",
    "\n",
    "        unwrapped = ref + delta\n",
    "        com = np.mean(unwrapped, axis=0)\n",
    "\n",
    "        # Wrap CoM into [-L/2, L/2]\n",
    "        com[0] = (com[0] + Lx/2) % Lx - Lx/2\n",
    "        com[1] = (com[1] + Ly/2) % Ly - Ly/2\n",
    "        com[2] = (com[2] + Lz/2) % Lz - Lz/2\n",
    "\n",
    "        flake_coms.append(com)\n",
    "\n",
    "    return np.array(flake_coms)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a4b5b1-3a0a-499c-a4f8-a2b0fdf2eaaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_clusters(flake_coms, box_lengths, cutoff, include_singletons=True):\n",
    "    Lx, Ly, Lz = box_lengths\n",
    "    n = len(flake_coms)\n",
    "    adjacency = [set() for _ in range(n)]\n",
    "\n",
    "    for i in range(n):\n",
    "        for j in range(i + 1, n):\n",
    "            dr = flake_coms[j] - flake_coms[i]\n",
    "            dr[0] -= Lx * np.round(dr[0] / Lx)\n",
    "            dr[1] -= Ly * np.round(dr[1] / Ly)\n",
    "            dr[2] -= Lz * np.round(dr[2] / Lz)\n",
    "            if np.linalg.norm(dr) < cutoff:\n",
    "                adjacency[i].add(j)\n",
    "                adjacency[j].add(i)\n",
    "\n",
    "    visited = set()\n",
    "    clusters = []\n",
    "\n",
    "    for i in range(n):\n",
    "        if i not in visited:\n",
    "            cluster = []\n",
    "            stack = [i]\n",
    "            while stack:\n",
    "                cur = stack.pop()\n",
    "                if cur not in visited:\n",
    "                    visited.add(cur)\n",
    "                    cluster.append(cur)\n",
    "                    stack.extend(adjacency[cur] - visited)\n",
    "\n",
    "            if include_singletons or len(cluster) > 1:\n",
    "                clusters.append(cluster)\n",
    "\n",
    "    return len(clusters), [len(c) for c in clusters], clusters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2a8371-8fc0-44f7-ae7f-c004eb1771a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_clusters_over_time(traj, start=0, step=1000, cutoff=2.5, beads_per_flake=50):\n",
    "    times = []\n",
    "    n_clusters_list = []\n",
    "    avg_cluster_sizes = []\n",
    "\n",
    "    for frame_idx in range(start, len(traj), step):\n",
    "        frame = traj[frame_idx]\n",
    "        box_lengths = frame.configuration.box[:3]\n",
    "\n",
    "        flake_type = \"F\"\n",
    "        flake_typeid = frame.particles.types.index(flake_type)\n",
    "        positions = frame.particles.position\n",
    "        typeids = frame.particles.typeid\n",
    "        flake_positions = positions[typeids == flake_typeid]\n",
    "\n",
    "        flake_coms = compute_all_flake_coms(flake_positions, box_lengths, beads_per_flake)\n",
    "        n_clusters, sizes, _ = count_clusters(flake_coms, box_lengths, cutoff)\n",
    "\n",
    "        times.append(frame_idx)\n",
    "        n_clusters_list.append(n_clusters)\n",
    "        avg_cluster_sizes.append(np.mean(sizes) if sizes else 0)\n",
    "\n",
    "    return times, n_clusters_list, avg_cluster_sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133b5518-c37d-4744-9685-b1d58a7f6158",
   "metadata": {},
   "source": [
    "# Adjust to whichever file you're trying to analyze, then hit run until the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02546b6-1ec3-46cb-b8d7-f680e694fb22",
   "metadata": {},
   "outputs": [],
   "source": [
    "name_start = \"140_10mer5f_0.005dt_1.0kT_0.001id_0.16fd_500000.0ssteps_1000000.0rsteps\"\n",
    "start_file = f\"{name_start}_start.txt\"\n",
    "traj_file = f\"{name_start}.gsd\"\n",
    "with open(start_file) as f:\n",
    "    print(f.readlines()[0].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330a659a-e793-441f-8467-ece43451d4a8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open(start_file) as f:\n",
    "    start = int(f.readlines()[0].strip()) # extracting post-shrink sim beginning frame\n",
    "traj = gsd.hoomd.open(traj_file)\n",
    "times, n_clusters, avg_sizes = analyze_clusters_over_time(\n",
    "    traj,\n",
    "    start=start,   # post-shrink\n",
    "    step=1, # analyze every \"x\" frame, leave this at 1\n",
    "    cutoff=2.5\n",
    ") # this function is used to initially calculate the decorrelation time, so that we know every how many certain amount of frames to analyze. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7217fadb-1e36-4144-856e-5e0ed85d6a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(times, n_clusters, marker='o')\n",
    "plt.title(\"Number of Clusters Over Time\")\n",
    "plt.xlabel(\"Frame\")\n",
    "plt.ylabel(\"Cluster Count\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(times, avg_sizes, marker='s', color='orange')\n",
    "plt.title(\"Average Cluster Size Over Time\")\n",
    "plt.xlabel(\"Frame\")\n",
    "plt.ylabel(\"Avg. Cluster Size\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c508c3-f7ad-459f-a824-3efa5d91e7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def autocorr1D(array):\n",
    "    ft = np.fft.rfft(array - np.average(array))  \n",
    "    acorr = np.fft.irfft(ft * np.conjugate(ft))  \n",
    "    acorr /= (len(array) * np.var(array))        \n",
    "\n",
    "    dt = np.where(acorr < 0)[0][0]               \n",
    "    nsamples = len(array) // dt                  \n",
    "\n",
    "    acorr = acorr[dt:nsamples]                  \n",
    "\n",
    "    return nsamples, dt\n",
    "n, dt = autocorr1D(avg_sizes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9d7575-7794-4b3d-af76-3effed81d4fd",
   "metadata": {},
   "source": [
    "# Decorrelation time is now calculated; now we can plot all the independent samples present within this simulation. Lower independent samples means more average aggregation over time is occurring, should occur as kT is increased. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791c5fa4-2804-45ca-bbb2-274f7e51a8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Independent samples and decorrelation time:\", n, dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0afd4b-ae71-486b-9900-600aa341a006",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we can plot every \"dt\"th frame, whose length should be ~= independent samples\n",
    "decorrelated_times = times[::dt]\n",
    "decorrelated_sizes = avg_sizes[::dt]\n",
    "decorrelated_clusters = n_clusters[::dt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c865ec-ca92-4738-aa53-d258b776cd29",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(decorrelated_sizes)) # verifying it is equal to calculated independent sampleses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "614f5d74-3842-4289-944d-5134ba76a6e0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(decorrelated_times, decorrelated_sizes) # plot of every independent sample in simulation\n",
    "plt.title(\"Average Cluster Size (Decorrelated)\")\n",
    "plt.xlabel(\"Frame\")\n",
    "plt.ylabel(\"Avg Cluster Size\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95d53e0-3cd4-4f94-8b6b-1a1ea37ed17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(decorrelated_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ca0f50-bbe1-40f8-b70f-8a56826ab11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(decorrelated_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06022fb2-060b-48d2-bf13-aa71836a49a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(decorrelated_times, decorrelated_clusters)\n",
    "plt.title(\"Number of Clusters (Decorrelated)\")\n",
    "plt.xlabel(\"Frame\")\n",
    "plt.ylabel(\"# Clusters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07a0779-ccb1-423c-b1d5-8d9f24481cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''# Add additional data to start_file\n",
    "decorrelated_values = [len(decorrelated_sizes), np.mean(decorrelated_sizes), np.mean(decorrelated_clusters)]\n",
    "values_to_add = [str(val) + '\\n' for val in decorrelated_values]\n",
    "with open(start_file, 'a') as f:\n",
    "    f.write(str(len(decorrelated_sizes)) + '\\n')\n",
    "    f.write(str(f\"{np.mean(decorrelated_sizes):.2f}\") + '\\n')\n",
    "    f.write(str(f\"{np.mean(decorrelated_clusters):.2f}\") + '\\n')'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8877081-10cb-41d3-b053-c2b104fdb496",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''# Add to Google Sheet\n",
    "service_account_file = \"../service_worker1.json\"\n",
    "\n",
    "gc = gspread.service_account(filename=service_account_file)\n",
    "\n",
    "spreadsheet = gc.open(\"Mxene Project Data\")\n",
    "worksheet = spreadsheet.worksheet(\"Week 11 Data\")\n",
    "data = worksheet.get_all_values()\n",
    "print(f\"Successfully read {len(data)} rows from the sheet.\")\n",
    "\n",
    "with open(start_file, 'r') as f:\n",
    "    new_row_data = f.readlines()[1:]\n",
    "\n",
    "worksheet.append_row(new_row_data)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1215ddfe-48d4-4cfe-a276-59d8081e20aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1801f02f-83e9-437e-9591-b421c8d24154",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3693b0-7054-4058-8653-9a89e83e1cda",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
